{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb0a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35bfa861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation of doc object based on unicode string input\n",
    "doc = nlp(u'I have flown to La. Now I am flying to San Fransisco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e58f1f87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flown']\n",
      "['flying']\n"
     ]
    }
   ],
   "source": [
    "#shred the discourse = separate into sentences\n",
    "for sent in doc.sents:\n",
    "    print([w.text for w in sent if w.dep_ == 'ROOT' or w.dep_ == 'probj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61268e66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I , PRP , PRON , nsubj , flown\n",
      "have , VBP , AUX , aux , flown\n",
      "flown , VBN , VERB , ROOT , flown\n",
      "to , IN , ADP , prep , flown\n",
      "La. , NNP , PROPN , punct , flown\n",
      "Now , RB , ADV , advmod , flying\n",
      "I , PRP , PRON , nsubj , flying\n",
      "am , VBP , AUX , aux , flying\n",
      "flying , VBG , VERB , ROOT , flying\n",
      "to , IN , ADP , prep , flying\n",
      "San , NNP , PROPN , compound , Fransisco\n",
      "Fransisco , NNP , PROPN , pobj , to\n"
     ]
    }
   ],
   "source": [
    "#examining the attributes of the doc's tokens\n",
    "for token in doc:\n",
    "    print(token.text, \",\", token.tag_, \",\", token.pos_, ',', token.dep_, \",\", token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeaa18f",
   "metadata": {},
   "source": [
    "# Try This Ch1:\n",
    "\n",
    "Create a script which properly inteprets the intent to fly to SanFran in the above doc.\n",
    "\n",
    "\"Start with latest script... enahance conditional clause adding conditions to account for fine-grained part-of-speech tags... on pg21. Then add lemmatization funtionality to script... on page 18.\"\n",
    "\n",
    "Output should be ['fly, 'San Fransico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b198e890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly I\n",
      "fly have\n",
      "fly flown\n",
      "fly to\n",
      "fly La.\n",
      "fly Now\n",
      "fly I\n",
      "fly am\n",
      "fly flying\n",
      "fly to\n",
      "to Fransisco\n"
     ]
    }
   ],
   "source": [
    "#attempt 1\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        if (token.dep_ == 'dobj' or token.dep_ == \"pobj\") and \\\n",
    "        (token.tag_ == 'NNS' or token.tag_ == 'NN' or token.tag_ == 'NNP') or \\\n",
    "        (token.head.tag_ == 'VB' or token.head.tag_ == 'VBG' or token.head.tag_ == 'VBN'):\n",
    "            print(token.head.lemma_, token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a3aef44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['fly', 'Fransisco']\n"
     ]
    }
   ],
   "source": [
    "#attempt 2\n",
    "for sent in doc.sents:\n",
    "    print([token.lemma_ for token in sent \\\n",
    "           if(token.dep_ == 'ROOT' or token.dep_ == 'pobj') and \\\n",
    "           (token.tag_ == 'NNS' or token.tag_ == 'NN' or token.tag_ == 'NNP' or\\\n",
    "            token.tag_ == 'VB' or token.tag_ == 'VBD' or token.tag_ == 'VBG')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f79dee",
   "metadata": {},
   "source": [
    "**The above was it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fbf7ab",
   "metadata": {},
   "source": [
    "# Try this #1 Ch2:\n",
    "Get noun chunks using tokens' syntatctic children instead of doc.noun_chunks. Utilize the .left container attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b117bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A noun chunk is a phrase that has a noun at its head."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'A noun chunk is a phrase that has a noun at its head.')\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65822160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A noun chunk\n",
      "a phrase\n",
      "that\n",
      "a noun\n",
      "its head\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79cfce66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " noun\n",
      " A noun chunk\n",
      " a phrase\n",
      " a noun\n",
      " its head\n"
     ]
    }
   ],
   "source": [
    "#try this\n",
    "for token in doc:\n",
    "    noun_chunk = ''\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        for child in token.lefts: #note: cannot access .lefts via indicies\n",
    "            noun_chunk = noun_chunk + \" \" + child.text\n",
    "        noun_chunk = noun_chunk + \" \" + token.text\n",
    "        print(noun_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2a955d",
   "metadata": {},
   "source": [
    "**Gottem**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf58614",
   "metadata": {},
   "source": [
    "# Try this 2 Ch2:\n",
    "Use a span to create a custom lemma of a multi word name, San Fransisco, to be a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "975ae219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The, Golden, Gate, Bridge, is, an, iconic, landmark, in, San, Fransisco, .]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#demonstration of initial tokens\n",
    "doc = nlp(u'The Golden Gate Bridge is an iconic landmark in San Fransisco.')\n",
    "[doc[i] for i in range(len(doc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e883cdd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The, Golden Gate Bridge, is, an, iconic, landmark, in, San, Fransisco, .]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'The Golden Gate Bridge is an iconic landmark in San Fransisco.')\n",
    "span = doc[1:4]\n",
    "lem_id = doc.vocab.strings[span.text] #doc.vocab.strings = a huge list of strings included in en_core_web_sm\n",
    "#merge golden gate bridge into 1 token\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(span)\n",
    "[doc[i] for i in range(len(doc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1ee4ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The, Golden Gate Bridge, is, an, iconic, landmark, in, San Fransisco, .]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge san fransisco into 1 token\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc[-3:-1])\n",
    "[doc[i] for i in range(len(doc))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5edcf5",
   "metadata": {},
   "source": [
    "**Got it**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae03e83f",
   "metadata": {},
   "source": [
    "# Try this 1 Ch4:\n",
    "\n",
    "Create a script to extract every phrase reffering to an amount of money in a given string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3a91e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The firm earned $1.5 million in 2017, in comparison with $1.2 million in 2016."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u\"The firm earned $1.5 million in 2017, in comparison with $1.2 million in 2016.\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc5c2469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 The DET DT\n",
      "1 firm NOUN NN\n",
      "2 earned VERB VBD\n",
      "3 $ SYM $\n",
      "4 1.5 NUM CD\n",
      "5 million NUM CD\n",
      "6 in ADP IN\n",
      "7 2017 NUM CD\n",
      "8 , PUNCT ,\n",
      "9 in ADP IN\n",
      "10 comparison NOUN NN\n",
      "11 with ADP IN\n",
      "12 $ SYM $\n",
      "13 1.2 NUM CD\n",
      "14 million NUM CD\n",
      "15 in ADP IN\n",
      "16 2016 NUM CD\n",
      "17 . PUNCT .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.i, token.text, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ab369d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$1.5 million \n",
      "$1.2 million \n"
     ]
    }
   ],
   "source": [
    "#get all references to monetary values from text\n",
    "phrase = ''\n",
    "for token in doc:\n",
    "    phrase = ''\n",
    "    i = token.i\n",
    "    if token.tag_ == '$':\n",
    "        phrase = '$'\n",
    "        i += 1\n",
    "        while doc[i].tag_ == 'CD':\n",
    "            phrase += doc[i].text + ' '\n",
    "            i += 1\n",
    "    if phrase != '':\n",
    "        print(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04160fc4",
   "metadata": {},
   "source": [
    "**Got it**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a47cf2",
   "metadata": {},
   "source": [
    "# CH 4 trying out the question thinggy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9c0a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import sys\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e422966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method determines what question type a chatbot would need to respond with based on the user input\n",
    "def find_chunk(doc):\n",
    "    chunk = ''\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.dep_ == \"dobj\":\n",
    "            shift = len([w for w in token.children])  #this only takes the leftmost children, not always the case\n",
    "            chunk = doc[i-shift:i+1]\n",
    "            break\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abdf1385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a strange steam controller"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('I have a strange steam controller.')\n",
    "find_chunk(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15c1a25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "What"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('What do you do with that big hammer?')\n",
    "find_chunk(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b795457d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "you"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('Can I confuse you?')\n",
    "find_chunk(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba8e5b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the pies"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('He ate the pies.')\n",
    "find_chunk(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a6b989",
   "metadata": {},
   "source": [
    "It seems as though some types of sentences are not suitable for this algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fa6d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_question_type(chunk):\n",
    "    question_type = 'yesno'\n",
    "    for token in chunk:\n",
    "        if token.dep_ == 'amod':\n",
    "            question_type = 'info'\n",
    "            break\n",
    "    return question_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d54eb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yesno'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('I like beans.')\n",
    "determine_question_type(find_chunk(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a14eb3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'info'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('I like red beans.')\n",
    "determine_question_type(find_chunk(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876df675",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('What do you do with that big hammer?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d833d5ec",
   "metadata": {},
   "source": [
    "# Intention Recognition\n",
    "\n",
    "Code from the book, ch 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73152cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a9f4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tv_dobj(doc):\n",
    "    intent = ''\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'dobj':\n",
    "            intent = token.head.text.lower() + token.text.capitalize()\n",
    "            return intent\n",
    "    return 'unknown' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c7c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_extract(string):\n",
    "    doc = nlp(string)\n",
    "    return extract_tv_dobj(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e542310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unknown'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('I want to go to bed.')\n",
    "extract_tv_dobj(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "989f48fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I nsubj\n",
      "want ROOT\n",
      "to aux\n",
      "go xcomp\n",
      "to prep\n",
      "bed pobj\n",
      ". punct\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e57dd1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'showSomething'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_extract('Show me something cool.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be643fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'haveVerbs'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_extract('does this sentence have two verbs?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e73416b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wantRefund'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_extract('I want a refund')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
